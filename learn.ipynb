{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# In Pydantic\n",
    "\n",
    "\n",
    "\n",
    "# # LangGraph's State Update Mechanism:\n",
    "\n",
    "In LangGraph node functions, you must return dictionaries regardless of whether you use TypedDict or Pydantic for the State definition. This is how LangGraph's state update mechanism works.\n",
    "\n",
    "\n",
    "Node receives state (as Pydantic object or dict)\n",
    "Node returns dictionary with updates\n",
    "LangGraph merges the returned dict with existing state\n",
    "add_messages function handles message list merging\n",
    "\n",
    "The Rule:\n",
    "\n",
    "Graph Input: Can be State object OR dictionary\n",
    "Node Functions: MUST return dictionaries\n",
    "Graph Output: Always dictionary\n",
    "\n",
    "```python\n",
    "# 1. Input to graph (can be object or dict)\n",
    "initial_state = State(messages=[...])  # OR {\"messages\": [...]}\n",
    "\n",
    "# 2. Inside node1\n",
    "def node1(state: State):  # Receives State object\n",
    "    return {\"messages\": [...]}  # MUST return dict\n",
    "\n",
    "# 3. LangGraph merges the dict with existing state\n",
    "# 4. Next node receives updated state\n",
    "def tool_calling_llm(state: State):  # Receives updated State\n",
    "    return {\"messages\": [...]}  # MUST return dict\n",
    "\n",
    "# 5. Final output from graph\n",
    "result = graph.invoke(initial_state)  # Always returns dict\n",
    "```\n",
    "\n",
    "So you are using the simple dictionary approach where it's required (in node return values) - this is the only way LangGraph node functions can work!\n",
    "\n",
    "\n",
    "## partial Updates\n",
    "\n",
    "```python\n",
    "# Update only messages, leave other fields unchanged\n",
    "def update_messages_only(state: State):\n",
    "    return {\"messages\": [new_message]}\n",
    "    # tool_input and tool_output remain unchanged!\n",
    "\n",
    "# Update only tool_input\n",
    "def update_tool_only(state: State):\n",
    "    return {\"tool_input\": \"new value\"}\n",
    "    # messages and tool_output remain unchanged!\n",
    "\n",
    "# Update multiple fields\n",
    "def update_multiple(state: State):\n",
    "    return {\n",
    "        \"messages\": [new_message],\n",
    "        \"tool_input\": \"updated\"\n",
    "    }\n",
    "    # tool_output remains unchanged!\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Node functions must return dictionaries because:\n",
    "\n",
    "LangGraph's architecture expects dictionary keys to know what to update\n",
    "Reducer functions like add_messages need to be called on specific fields\n",
    "Partial updates are only possible with dictionary keys\n",
    "Type safety is maintained while allowing flexible updates\n",
    "Framework design - this is how LangGraph fundamentally works\n",
    "\n",
    "It's not about preference - it's about how LangGraph's state management system is architected!\n"
   ],
   "id": "ae2e3641fd6211d8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage\n",
    "from typing import Annotated, List\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "import os, getpass\n",
    "from pprint import pprint\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "def llm():\n",
    "    return ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# ✅ Use Pydantic BaseModel\n",
    "class State(BaseModel):\n",
    "    messages: Annotated[List[BaseMessage], add_messages] = Field(default_factory=list)\n",
    "    tool_input: str = Field(default=\"\")\n",
    "    tool_output: str = Field(default=\"\")\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True  # Allow BaseMessage types\n",
    "\n",
    "@tool\n",
    "def multiply(x: int, y: int) -> int:\n",
    "    \"\"\"Multiply two integers.\n",
    "\n",
    "    Args:\n",
    "        x: First integer\n",
    "        y: Second integer\n",
    "    \"\"\"\n",
    "    return x * y\n",
    "\n",
    "def node1(state: State):\n",
    "    \"\"\"Initialize conversation if no messages exist.\"\"\"\n",
    "    # Check if messages already exist\n",
    "    return {\n",
    "            \"messages\": [HumanMessage(content=\"What is 4 times 4\")],\n",
    "            \"tool_input\": \"\",\n",
    "            \"tool_output\": \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "def tool_calling_llm(state: State):\n",
    "    \"\"\"LLM node that can call tools.\"\"\"\n",
    "    tools = [multiply]\n",
    "    llm_with_tools = llm().bind_tools(tools)\n",
    "    # ✅ Use dot notation with Pydantic BaseModel\n",
    "    return {\"messages\": [llm_with_tools.invoke(state.messages)]}\n",
    "\n",
    "def tool_execution_node(state: State):\n",
    "    \"\"\"Execute tools based on the LLM's tool calls.\"\"\"\n",
    "    tool_node = ToolNode([multiply])\n",
    "    # Convert state to dict for ToolNode\n",
    "    state_dict = {\n",
    "        \"messages\": state.messages,\n",
    "        \"tool_input\": state.tool_input,\n",
    "        \"tool_output\": state.tool_output\n",
    "    }\n",
    "    return tool_node.invoke(state_dict)\n",
    "\n",
    "def should_continue(state: State):\n",
    "    \"\"\"Determine if we should continue to tools or end.\"\"\"\n",
    "    messages = state.messages  # ✅ Use dot notation\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # Check if the last message has tool calls\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"node1\", node1)\n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "builder.add_node(\"tools\", tool_execution_node)\n",
    "\n",
    "builder.set_entry_point(\"node1\")\n",
    "builder.add_edge(\"node1\", \"tool_calling_llm\")\n",
    "\n",
    "# Add conditional edge to handle tool calling\n",
    "builder.add_conditional_edges(\n",
    "    \"tool_calling_llm\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# After tools, go back to LLM for final response\n",
    "builder.add_edge(\"tools\", \"tool_calling_llm\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# LangGraph Return Type\n",
    "# Input: Can be Pydantic object or dict\n",
    "# Output: Always returns a dictionary\n",
    "# Reason: Internal processing converts everything to dicts\n",
    "\n",
    "\n",
    "result_dict = graph.invoke({\n",
    "    \"messages\": [SystemMessage(content=\"You are a mathematician\"),HumanMessage(content=\"What is 5 times 6?\")],\n",
    "    \"tool_input\": \"\",\n",
    "    \"tool_output\": \"\"\n",
    "})\n",
    "\n",
    "print(\"Final result:\")\n",
    "print(result_dict)\n",
    "\n",
    "for m in result_dict[\"messages\"]:\n",
    "    m.pretty_print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "pprint(result_dict[\"messages\"][-1].content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# In TypedDict",
   "id": "409252d4026fe5f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage\n",
    "from typing import Annotated, TypedDict  # ✅ Import TypedDict instead of BaseModel\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "import os, getpass\n",
    "from pprint import pprint\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "def llm():\n",
    "    return ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# ✅ Use TypedDict instead of Pydantic BaseModel\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    tool_input: str\n",
    "    tool_output: str\n",
    "\n",
    "@tool\n",
    "def multiply(x: int, y: int) -> int:\n",
    "    \"\"\"Multiply two integers.\n",
    "\n",
    "    Args:\n",
    "        x: First integer\n",
    "        y: Second integer\n",
    "    \"\"\"\n",
    "    return x * y\n",
    "\n",
    "def node1(state: State):\n",
    "    \"\"\"Initialize conversation if no messages exist.\"\"\"\n",
    "    # Check if messages already exist\n",
    "    return {\n",
    "            \"messages\": [HumanMessage(content=\"What is 4 times 4\")],\n",
    "            \"tool_input\": \"\",\n",
    "            \"tool_output\": \"\",\n",
    "    }\n",
    "\n",
    "def tool_calling_llm(state: State):\n",
    "    \"\"\"LLM node that can call tools.\"\"\"\n",
    "    tools = [multiply]\n",
    "    llm_with_tools = llm().bind_tools(tools)\n",
    "    # ✅ Use dictionary notation with TypedDict\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "def tool_execution_node(state: State):\n",
    "    \"\"\"Execute tools based on the LLM's tool calls.\"\"\"\n",
    "    tool_node = ToolNode([multiply])\n",
    "    # No need to convert state - it's already a dict\n",
    "    return tool_node.invoke(state)\n",
    "\n",
    "def should_continue(state: State):\n",
    "    \"\"\"Determine if we should continue to tools or end.\"\"\"\n",
    "    messages = state[\"messages\"]  # ✅ Use dictionary notation\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # Check if the last message has tool calls\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"node1\", node1)\n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "builder.add_node(\"tools\", tool_execution_node)\n",
    "\n",
    "builder.set_entry_point(\"node1\")\n",
    "builder.add_edge(\"node1\", \"tool_calling_llm\")\n",
    "\n",
    "# Add conditional edge to handle tool calling\n",
    "builder.add_conditional_edges(\n",
    "    \"tool_calling_llm\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# After tools, go back to LLM for final response\n",
    "builder.add_edge(\"tools\", \"tool_calling_llm\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "result_dict = graph.invoke({\n",
    "    \"messages\": [SystemMessage(content=\"You are a mathematician\"), HumanMessage(content=\"What is 5 times 6?\")],\n",
    "    \"tool_input\": \"\",\n",
    "    \"tool_output\": \"\"\n",
    "})\n",
    "\n",
    "print(\"Final result:\")\n",
    "print(result_dict)\n",
    "\n",
    "for m in result_dict[\"messages\"]:\n",
    "    m.pretty_print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "pprint(result_dict[\"messages\"][-1].content)\n",
    "\n",
    "# Additional example showing TypedDict benefits\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== TYPEDDICT BENEFITS ===\")\n",
    "\n",
    "# ✅ Simple dictionary creation (no class instantiation needed)\n",
    "simple_state = {\n",
    "    \"messages\": [\n",
    "        SystemMessage(content=\"You are a helpful calculator.\"),\n",
    "        HumanMessage(content=\"What is 10 times 3?\")\n",
    "    ],\n",
    "    \"tool_input\": \"\",\n",
    "    \"tool_output\": \"\"\n",
    "}\n",
    "\n",
    "result_simple = graph.invoke(simple_state)\n",
    "print(\"Simple dictionary invocation works perfectly!\")\n",
    "print(f\"Answer: {result_simple['messages'][-1].content}\")\n",
    "\n",
    "# ✅ No need for special Config or Field imports\n",
    "# ✅ Consistent dictionary access throughout\n",
    "# ✅ Better performance (no validation overhead)"
   ],
   "id": "6ce8bfcc41fdf47d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T02:25:20.488248Z",
     "start_time": "2025-08-13T02:25:20.478023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class state(BaseModel):\n",
    "    messages: Annotated[List[BaseMessage], add_messages] = Field(default_factory=list)\n",
    "    tool_input: str = Field(default=\"\")\n",
    "    tool_output: str = Field(default=\"\")\n",
    "\n",
    "\n",
    "\n",
    "def Node1(state: state):\n",
    "    \"\"\"Initialize conversation if no messages exist.\"\"\"\n",
    "    # Check if messages already exist\n",
    "    return state.model_copy(update={\n",
    "        \"messages\": [HumanMessage(content=\"What is 4 times 4\")],\n",
    "    })\n",
    "\n",
    "\n"
   ],
   "id": "bea8323fc9af6738",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function node1 at 0x10e5e7a60>\n"
     ]
    }
   ],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
