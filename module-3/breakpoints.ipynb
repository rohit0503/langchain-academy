{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1012a788",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-3/breakpoints.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239469-lesson-2-breakpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa16f5-abc8-4ed3-8a71-54837fe46917",
   "metadata": {},
   "source": [
    "# Breakpoints\n",
    "\n",
    "## Review\n",
    "\n",
    "For `human-in-the-loop`, we often want to see our graph outputs as its running. \n",
    "\n",
    "We laid the foundations for this with streaming. \n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, let's talk about the motivations for `human-in-the-loop`:\n",
    "\n",
    "(1) `Approval` - We can interrupt our agent, surface state to a user, and allow the user to accept an action\n",
    "\n",
    "(2) `Debugging` - We can rewind the graph to reproduce or avoid issues\n",
    "\n",
    "(3) `Editing` - You can modify the state \n",
    "\n",
    "LangGraph offers several ways to get or update agent state to support various `human-in-the-loop` workflows.\n",
    "\n",
    "First, we'll introduce [breakpoints](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/breakpoints/#simple-usage), which provide a simple way to stop the graph at specific steps. \n",
    "\n",
    "We'll show how this enables user `approval`."
   ]
  },
  {
   "cell_type": "code",
   "id": "35842345-0694-4f0a-aa62-7d4898abf653",
   "metadata": {},
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai langgraph_sdk langgraph-prebuilt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "67d91f7c",
   "metadata": {},
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "31d8b4cd-e3ff-48cc-b7b2-f83fadb1c86b",
   "metadata": {},
   "source": [
    "## Breakpoints for human approval\n",
    "\n",
    "Let's re-consider the simple agent that we worked with in Module 1. \n",
    "\n",
    "Let's assume that are concerned about tool use: we want to approve the agent to use any of its tools.\n",
    " \n",
    "All we need to do is simply compile the graph with `interrupt_before=[\"tools\"]` where `tools` is our tools node.\n",
    "\n",
    "This means that the execution will be interrupted before the node `tools`, which executes the tool call."
   ]
  },
  {
   "cell_type": "code",
   "id": "b94d1a90-2fe3-4b2a-a901-3bdb89e37edc",
   "metadata": {},
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "# This will be a tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a by b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "tools = [add, multiply, divide]\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ac06feae-d12b-490b-95e7-38cf40b74202",
   "metadata": {},
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# System message\n",
    "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
    "\n",
    "# Node\n",
    "def assistant(state: MessagesState):\n",
    "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
    "\n",
    "# Graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine the control flow\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=[\"tools\"], checkpointer=memory)\n",
    "\n",
    "# Show\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a783efac-46a9-4fb4-a1c6-a11b02540448",
   "metadata": {},
   "source": [
    "# Input\n",
    "initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}\n",
    "\n",
    "# Thread\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    event['messages'][-1].pretty_print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "50d49669-b1a5-42c2-bdb8-052da89bd7c4",
   "metadata": {},
   "source": [
    "We can get the state and look at the next node to call.\n",
    "\n",
    "This is a nice way to see that the graph has been interrupted."
   ]
  },
  {
   "cell_type": "code",
   "id": "61569596-8342-4a37-9c99-e3a9dccb18ee",
   "metadata": {},
   "source": [
    "state = graph.get_state(thread)\n",
    "state\n",
    "# state.next"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fea0fb5-3145-4f34-bcc0-9c9e8972d6b4",
   "metadata": {},
   "source": [
    "Now, we'll introduce a nice trick.\n",
    "\n",
    "When we invoke the graph with `None`, it will just continue from the last state checkpoint!\n",
    "\n",
    "![breakpoints.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbae7985b747dfed67775d_breakpoints1.png)\n",
    "\n",
    "For clarity, LangGraph will re-emit the current state, which contains the `AIMessage` with tool call.\n",
    "\n",
    "And then it will proceed to execute the following steps in the graph, which start with the tool node.\n",
    "\n",
    "We see that the tool node is run with this tool call, and it's passed back to the chat model for our final answer."
   ]
  },
  {
   "cell_type": "code",
   "id": "896a5f41-7386-4bfa-a78e-3e6ca5e26641",
   "metadata": {},
   "source": [
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    event['messages'][-1].pretty_print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "37f91a0c-7cc1-4437-adc7-b36abb29beb1",
   "metadata": {},
   "source": [
    "Now, lets bring these together with a specific user approval step that accepts user input."
   ]
  },
  {
   "cell_type": "code",
   "id": "95a0eb50-66e3-4538-8103-207aae175154",
   "metadata": {},
   "source": [
    "# Input\n",
    "initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}\n",
    "\n",
    "# Thread\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    event['messages'][-1].pretty_print()\n",
    "\n",
    "# Get user feedback\n",
    "user_approval = input(\"Do you want to call the tool? (yes/no): \")\n",
    "\n",
    "# Check approval\n",
    "if user_approval.lower() == \"yes\":\n",
    "    \n",
    "    # If approved, continue the graph execution\n",
    "    for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "        event['messages'][-1].pretty_print()\n",
    "        \n",
    "else:\n",
    "    print(\"Operation cancelled by user.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8ff8762-6fa1-4373-954a-e7f479ee0efb",
   "metadata": {},
   "source": [
    "### Breakpoints with LangGraph API\n",
    "\n",
    "**âš ï¸ DISCLAIMER**\n",
    "\n",
    "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "You should see the following output:\n",
    "```\n",
    "- ðŸš€ API: http://127.0.0.1:2024\n",
    "- ðŸŽ¨ Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "- ðŸ“š API Docs: http://127.0.0.1:2024/docs\n",
    "```\n",
    "\n",
    "Open your browser and navigate to the Studio UI: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`.\n",
    "\n",
    "The LangGraph API [supports breakpoints](https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_breakpoint/#sdk-initialization). "
   ]
  },
  {
   "cell_type": "code",
   "id": "63c2eaf1-6b8b-4d80-9902-98ae5587bcf9",
   "metadata": {},
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb1dd890-c216-4802-9e33-b637e491e144",
   "metadata": {},
   "source": [
    "# This is the URL of the local development server\n",
    "from langgraph_sdk import get_client\n",
    "client = get_client(url=\"http://127.0.0.1:2024\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1e80d969-d065-45d7-8bfc-a403a0a1079b",
   "metadata": {},
   "source": [
    "As shown above, we can add `interrupt_before=[\"node\"]` when compiling the graph that is running in Studio.\n",
    "\n",
    "However, with the API, you can also pass `interrupt_before` to the stream method directly. "
   ]
  },
  {
   "cell_type": "code",
   "id": "de9c5017-3a15-46f6-8edf-3997613da323",
   "metadata": {},
   "source": [
    "initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}\n",
    "thread = await client.threads.create()\n",
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input=initial_input,\n",
    "    stream_mode=\"values\",\n",
    "    interrupt_before=[\"tools\"],\n",
    "):\n",
    "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
    "    messages = chunk.data.get('messages', [])\n",
    "    if messages:\n",
    "        print(messages[-1])\n",
    "    print(\"-\" * 50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b64272d1-c6ee-435f-9890-9b6c3525ca6c",
   "metadata": {},
   "source": [
    "Now, we can proceed from the breakpoint just like we did before by passing the `thread_id` and `None` as the input!"
   ]
  },
  {
   "cell_type": "code",
   "id": "76284730-9c90-46c4-8295-400a49760b07",
   "metadata": {},
   "source": [
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    \"agent\",\n",
    "    input=None,\n",
    "    stream_mode=\"values\",\n",
    "    interrupt_before=[\"tools\"],\n",
    "):\n",
    "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
    "    messages = chunk.data.get('messages', [])\n",
    "    if messages:\n",
    "        print(messages[-1])\n",
    "    print(\"-\" * 50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4575970f-42e2-4d03-b18a-aacaa8233b53",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "\n",
    "## What LangGraph is (in 60 seconds)\n",
    "* Graphs, not chains. You model your app as a stateful directed graph: nodes are functions (or subgraphs), edges decide what runs next.\n",
    "* ingle shared state. Nodes read from and return partial updates to a shared state object (usually a TypedDict or Pydantic model). LangGraph merges those partial updates for you.\n",
    "* erministic control flow. You wire START â†’ nodes â†’ END. You can add conditional edges for branching and subgraphs for composition.\n",
    "* sistence + threads. Every step is checkpointed; runs are grouped into threads so you can pause/resume later and recover state safely. This is the foundation for human-in-the-loop.  ï¿¼\n",
    "\n",
    "## Core primitives (the mental model)\n",
    "\n",
    "1. State\n",
    "\n",
    "```python\n",
    "from typing import TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: list   # or any keys you need\n",
    "```\n",
    "Nodes return partial updates. If a node returns {\"messages\": [...]}, only messages changes.\n",
    "\n",
    "2. Nodes\n",
    "\n",
    "```python\n",
    "def plan(state: State) -> dict:\n",
    "    # read state, compute, return an update\n",
    "    return {\"messages\": state[\"messages\"] + [\"plan ready\"]}\n",
    "```\n",
    "\n",
    "3. Edges & compilation\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"plan\", plan)\n",
    "builder.add_edge(START, \"plan\")\n",
    "builder.add_edge(\"plan\", END)\n",
    "graph = builder.compile()\n",
    "```\n",
    "\n",
    "4. Running\n",
    "\n",
    "**chronous**: graph.invoke(inputs)\n",
    "**Streaming**: for chunk in graph.stream(inputs, stream_mode=...): ...\n",
    "\n",
    "## Streaming 101: stream_mode=\"values\" vs \"updates\"\n",
    "\n",
    "LangGraph can stream intermediate progress:\n",
    "* **\"values\"** â†’ after each step, you get the entire current state (full snapshot).\n",
    "* **\"dates\"** â†’ you get only the delta for that step (incremental updates). If a step runs multiple nodes, you can see multiple small updates.\n",
    "\n",
    "Typical usage youâ€™ll see in examples:\n",
    "\n",
    "```python\n",
    "for event in graph.stream(inputs, thread, stream_mode=\"values\"):\n",
    "    # event is the full state snapshot after each step\n",
    "\n",
    "for event in graph.stream(inputs, thread, stream_mode=\"updates\"):\n",
    "    # event is just the change(s) made in that step\n",
    "```\n",
    "\n",
    "### Rule of thumb:\n",
    "* Use updates for tight progress UIs (less data, easier to diff).\n",
    "*  values for debugging when you want to inspect the whole state each step.\n",
    "\n",
    "## Human-in-the-Loop (HITL): the key idea\n",
    "\n",
    "LangGraph lets you pause the graph at defined points, gather human input/approval, then resume exactly where you left off because state is checkpointed per thread. There are two main patterns:\n",
    "* **Declarative â€œbreakpointsâ€ around nodes**\n",
    "Add interrupt_before=[\"tools\"] (or specific node names) so the graph automatically pauses before those nodes run.\n",
    "* **grammatic interrupts inside a node**\n",
    "Call interrupt(...) in node code to pause with your own custom payload and wait for a resume.  ï¿¼ ï¿¼\n",
    "\n",
    "This is especially useful to approve tool calls (e.g., â€œSend email?â€, â€œBuy flight?â€) or to ask the user a clarifying question before continuing.\n",
    "\n",
    "\n",
    "### Mapping to your uploaded notebook (breakpoints.ipynb)ï¿¼\n",
    "\n",
    "Your notebook demonstrates two complementary flows:\n",
    "\n",
    "### Local graph streaming + manual approval\n",
    "    * builds an agent-like graph (LLM + tools), then runs:\n",
    "```python\n",
    "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    ...\n",
    "\n",
    "user_approval = input(\"Do you want to call the tool? (yes/no): \")\n",
    "if user_approval.lower() == \"yes\":\n",
    "    # resume the graph from the pause point\n",
    "    for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "        ...\n",
    "else:\n",
    "    print(\"Operation cancelled by user.\")\n",
    "```\n",
    "\n",
    "Whatâ€™s happening:\n",
    "    * First .stream(...) progresses until it hits the interrupt (configured to pause before running tools).\n",
    "    * You ask the human for approval.\n",
    "    * If approved, call .stream(None, thread, ...) again to resume. Because LangGraph persists checkpoints per thread, it continues from exactly where it paused.\n",
    "\n",
    "### Platform/SDK streaming with server-side runs\n",
    "\n",
    "The notebook also shows using the LangGraph SDK:\n",
    "```python\n",
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    \"agent\",\n",
    "    input=None,\n",
    "    stream_mode=\"values\",\n",
    "    interrupt_before=[\"tools\"],\n",
    "):\n",
    "    ...\n",
    "```\n",
    "\n",
    "    * Same concept, but now the graph is deployed; you stream server events, catch the interrupt event, collect input/approval in your appâ€™s UI, then resume the run.  ï¿¼\n",
    "\n",
    "These match the official â€œAdd human interventionâ€ and â€œHuman-in-the-loopâ€ docs: pause with interrupts, resume later using the threadâ€™s saved state.\n",
    "\n",
    "### what itâ€™s teaching you\n",
    "\n",
    "The â€œAdd human in the loop â†’ Simple usageâ€ guide shows:\n",
    "* Use interrupts to pause right before tools (or any node) so a human can review/edit.\n",
    "*  persistence layer (threads/checkpoints) ensures the agent can wait indefinitely and then continue from the exact step later.  ï¿¼\n",
    "\n",
    "Related, more detailed guides show:\n",
    "* Breakpoints: explicit pausing points and how theyâ€™re built on top of checkpoints/threads.  ï¿¼\n",
    "* grammatic interrupts: call interrupt() from inside a node to request user input and resume.  ï¿¼\n",
    "* ceptual overview + blog: why HITL exists and how interrupt evolved.\n",
    "\n",
    "\n",
    "### Minimal end-to-end example (glued together)\n",
    "\n",
    "```python\n",
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: list\n",
    "\n",
    "# 1) Define a simple tool\n",
    "def search_tool(query: str) -> str:\n",
    "    return f\"RESULTS({query})\"\n",
    "\n",
    "tools = [{\"name\": \"search\", \"description\": \"web search\", \"args\": {\"query\": \"str\"}, \"func\": search_tool}]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# 2) LLM node that may call the tool\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")  # example\n",
    "def agent(state: State):\n",
    "    # ask the model; bind tools so it can propose a tool call\n",
    "    res = llm.bind_tools(tools).invoke(state[\"messages\"])\n",
    "    return {\"messages\": state[\"messages\"] + [res]}\n",
    "\n",
    "# 3) Wire graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"agent\", agent)\n",
    "builder.add_node(\"tools\", tool_node)\n",
    "builder.add_edge(START, \"agent\")\n",
    "builder.add_edge(\"agent\", \"tools\")      # tool call proposed â†’ will run tools\n",
    "builder.add_edge(\"tools\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# 4) Run with a breakpoint BEFORE tools\n",
    "thread = {\"configurable\": {\"thread_id\": \"t1\"}}\n",
    "for ev in graph.stream({\"messages\":[HumanMessage(\"search latest LangGraph docs\")]},\n",
    "                       thread,\n",
    "                       stream_mode=\"updates\",     # or \"values\"\n",
    "                       interrupt_before=[\"tools\"] # â† pause here\n",
    "                       ):\n",
    "    print(ev)\n",
    "\n",
    "# ...show proposed tool call to a human...\n",
    "\n",
    "approved = True  # â† collect from UI\n",
    "if approved:\n",
    "    # resume from checkpoint; pass None input to continue\n",
    "    for ev in graph.stream(None, thread, stream_mode=\"updates\"):\n",
    "        print(ev)\n",
    "```\n",
    "The first stream(...) pauses before tools. You review/edit the tool call.\n",
    "* The second stream(None, thread, ...) resumes and runs the tool if approved.\n",
    "(Identical pattern works with the LangGraph SDK on a deployed graph.)  ï¿¼ ï¿¼\n",
    "\n",
    "ï¿¼\n",
    "\n",
    "Practical tips & gotchas\n",
    "* Choose a clear state model. Start with a small TypedDict and let nodes update only what they own. This keeps merges predictable.\n",
    "* Prefer updates for UI progress and logs; switch to values when debugging to see everything.  ï¿¼\n",
    "* Name your nodes meaningfully and interrupt selectively (e.g., interrupt_before=[\"tools\"] or a particular sensitive node).\n",
    "* Threads are your friend. Always run with a thread_id so you can pause/resume later without losing context.  ï¿¼\n",
    "* Programmatic interrupt() is perfect when you need to pause inside logic (e.g., â€œI need a date range from the userâ€).  ï¿¼\n",
    "* Server vs local. Locally, call graph.stream(...). When deployed, use client.runs.stream(...) with the same stream_mode and interrupt_before flags.\n",
    "\n",
    "\n",
    "What you should take away\n",
    "1. Model your app as nodes that return partial state updates; wire them with edges; compile.\n",
    "2. Use stream_mode=\"updates\" for progress UIs; \"values\" for full snapshots.  ï¿¼\n",
    "3. Enable HITL with interrupts (before specific nodes or from inside nodes) and resume using the same thread. This is reliable because LangGraph saves checkpoints every step.  ï¿¼ ï¿¼\n",
    "\n",
    "If youâ€™d like, I can turn your notebook into a cleaner, production-style template (CLI or Streamlit) that shows tool-approval with both local and SDK flowsâ€”just say the word.\n",
    "\n",
    "\n",
    "\n",
    "## Uisng breakpoints with Invoke\n",
    "\n",
    "```python\n",
    "\n",
    "from typing import TypedDict, Any\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint import interrupt\n",
    "\n",
    "# 1) Define your shared state\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    date_range: dict | None\n",
    "    status: str\n",
    "\n",
    "# 2) Node that pauses to ask a human for info\n",
    "def ask_for_dates(state: State) -> dict:\n",
    "    # Halt here and ask the operator for a date range.\n",
    "    # The *next* input you pass to graph.invoke(..., config=same_thread)\n",
    "    # will be returned as the value of `interrupt(...)`.\n",
    "    human_value = interrupt({\n",
    "        \"type\": \"request\",\n",
    "        \"message\": \"Please provide a date range like {'start':'YYYY-MM-DD','end':'YYYY-MM-DD'}.\"\n",
    "    })\n",
    "    # When resumed, `human_value` is whatever you passed to the next invoke.\n",
    "    return {\"date_range\": human_value, \"status\": \"got_dates\"}\n",
    "\n",
    "# 3) A follow-up node that uses the human-provided data\n",
    "def run_report(state: State) -> dict:\n",
    "    start = state[\"date_range\"][\"start\"]\n",
    "    end = state[\"date_range\"][\"end\"]\n",
    "    # ... do work (query, tool call, etc.) ...\n",
    "    return {\"status\": f\"report_done_for_{start}_to_{end}\"}\n",
    "\n",
    "# 4) Build & compile the graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"ask_for_dates\", ask_for_dates)\n",
    "builder.add_node(\"run_report\", run_report)\n",
    "builder.add_edge(START, \"ask_for_dates\")\n",
    "builder.add_edge(\"ask_for_dates\", \"run_report\")\n",
    "builder.add_edge(\"run_report\", END)\n",
    "graph = builder.compile()\n",
    "```\n",
    "\n",
    "How it works (mental model)\n",
    "* interrupt(payload) records a checkpoint and yields control back to the caller.\n",
    "*  first .invoke(initial_input, config) runs until it hits that interrupt and returns early with the partial state.\n",
    "*  next call â€” .invoke(human_input, same_config) â€” resumes the graph right after the interrupt line. Inside the node, interrupt(...) returns the human_input you just passed, so your node\n",
    "can keep going.\n",
    "\n",
    "If you have multiple interruptions in a row, each resume call satisfies the next pending interrupt(...).\n",
    "\n",
    "\n",
    "### Validate the human input (Pydantic)\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, field_validator\n",
    "\n",
    "class DateRange(BaseModel):\n",
    "    start: str\n",
    "    end: str\n",
    "    @field_validator(\"start\", \"end\")\n",
    "    @classmethod\n",
    "    def must_be_yyyy_mm_dd(cls, v):\n",
    "        assert len(v) == 10 and v[4] == \"-\" and v[7] == \"-\"\n",
    "        return v\n",
    "\n",
    "def ask_for_dates(state: State) -> dict:\n",
    "    value = interrupt({\"type\":\"request\",\"message\":\"Provide {'start','end'} in YYYY-MM-DD\"})\n",
    "    dr = DateRange(**value)  # raises if invalid\n",
    "    return {\"date_range\": dr.model_dump(), \"status\": \"got_dates\"}\n",
    "```\n",
    "\n",
    "### Multiple programmatic interrupts in one node\n",
    "\n",
    "```python\n",
    "def two_step_approval(state: State) -> dict:\n",
    "    first = interrupt({\"type\":\"approve\", \"message\":\"Approve running the query?\"})\n",
    "    if not first.get(\"approved\"):\n",
    "        return {\"status\": \"cancelled\"}\n",
    "\n",
    "    second = interrupt({\"type\":\"confirm\", \"message\":\"Are you sure? This may incur cost.\"})\n",
    "    if not second.get(\"approved\"):\n",
    "        return {\"status\": \"cancelled\"}\n",
    "\n",
    "    return {\"status\": \"approved\"}\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
