{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9e547f",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-3/streaming-interruption.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239464-lesson-1-streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319adfec-2d0a-49f2-87f9-275c4a32add2",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "## Review\n",
    "\n",
    "In module 2, covered a few ways to customize graph state and memory.\n",
    " \n",
    "We built up to a Chatbot with external memory that can sustain long-running conversations. \n",
    "\n",
    "## Goals\n",
    "\n",
    "This module will dive into `human-in-the-loop`, which builds on memory and allows users to interact directly with graphs in various ways. \n",
    "\n",
    "To set the stage for `human-in-the-loop`, we'll first dive into streaming, which provides several ways to visualize graph output (e.g., node state or chat model tokens) over the course of execution."
   ]
  },
  {
   "cell_type": "code",
   "id": "db024d1f-feb3-45a0-a55c-e7712a1feefa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T07:15:35.784347Z",
     "start_time": "2025-08-16T07:15:34.555448Z"
    }
   },
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai langgraph_sdk"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "70d7e41b-c6ba-4e47-b645-6c110bede549",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "LangGraph is built with [first class support for streaming](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
    "\n",
    "Let's set up our Chatbot from Module 2, and show various way to stream outputs from the graph during execution. "
   ]
  },
  {
   "cell_type": "code",
   "id": "5b430d92-f595-4322-a56e-06de7485daa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T07:15:54.400216Z",
     "start_time": "2025-08-16T07:15:50.504001Z"
    }
   },
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "4d0682fc",
   "metadata": {},
   "source": [
    "Note that we use `RunnableConfig` with `call_model` to enable token-wise streaming. This is [only needed with python < 3.11](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/). We include in case you are running this notebook in CoLab, which will use python 3.x. "
   ]
  },
  {
   "cell_type": "code",
   "id": "2d7321e0-0d99-4efe-a67b-74c12271859b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T07:16:00.756122Z",
     "start_time": "2025-08-16T07:15:59.366002Z"
    }
   },
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# LLM\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0) \n",
    "\n",
    "# State \n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "# Define the logic to call the model\n",
    "def call_model(state: State, config: RunnableConfig):\n",
    "    \n",
    "    # Get summary if it exists\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # If there is summary, then we add it\n",
    "    if summary:\n",
    "        \n",
    "        # Add summary to system message\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "\n",
    "        # Append summary to any newer messages\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    \n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "    response = model.invoke(messages, config)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    \n",
    "    # First, we get any existing summary\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # Create our summarization prompt \n",
    "    if summary:\n",
    "        \n",
    "        # A summary already exists\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    # Add prompt to our history\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
    "\n",
    "# Determine whether to end or summarize the conversation\n",
    "def should_continue(state: State):\n",
    "    \n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    \n",
    "    # Otherwise we can just end\n",
    "    return END\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# Set the entrypoint as conversation\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f847a787-b301-488c-9b58-cba9f389f55d",
   "metadata": {},
   "source": [
    "### Streaming full state\n",
    "\n",
    "Now, let's talk about ways to [stream our graph state](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
    "\n",
    "`.stream` and `.astream` are sync and async methods for streaming back results. \n",
    " \n",
    "LangGraph supports a few [different streaming modes](https://langchain-ai.github.io/langgraph/how-tos/stream-values/) for [graph state](https://langchain-ai.github.io/langgraph/how-tos/stream-values/):\n",
    " \n",
    "* `values`: This streams the full state of the graph after each node is called.\n",
    "* `updates`: This streams updates to the state of the graph after each node is called.\n",
    "\n",
    "![values_vs_updates.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf892d24625a201744e5_streaming1.png)\n",
    "\n",
    "Let's look at `stream_mode=\"updates\"`.\n",
    "\n",
    "Because we stream with `updates`, we only see updates to the state after node in the graph is run.\n",
    "\n",
    "Each `chunk` is a dict with `node_name` as the key and the updated state as the value."
   ]
  },
  {
   "cell_type": "code",
   "id": "9a6f8ae9-f244-40c5-a2da-618b72631b22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T07:16:10.938774Z",
     "start_time": "2025-08-16T07:16:07.334566Z"
    }
   },
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Lance\")]}, config, stream_mode=\"updates\"):\n",
    "    print(chunk)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation': {'messages': AIMessage(content='Hello Lance! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_80956533cb', 'id': 'chatcmpl-C55R4ZPcZ35sBsbBQCYRT2RaJQlv4', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ef98d75c-8884-46ed-bdfd-4c95eb1e870c-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "0c4882e9-07dd-4d70-866b-dfc530418cad",
   "metadata": {},
   "source": [
    "Let's now just print the state update."
   ]
  },
  {
   "cell_type": "code",
   "id": "c859c777-cb12-4682-9108-6b367e597b81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T07:16:13.907126Z",
     "start_time": "2025-08-16T07:16:12.819230Z"
    }
   },
   "source": [
    "# Start conversation\n",
    "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Lance\")]}, config, stream_mode=\"updates\"):\n",
    "    chunk['conversation'][\"messages\"].pretty_print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Hi Lance! How's it going? What can I do for you today?\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "583bf219-6358-4d06-ae99-c40f43569fda",
   "metadata": {},
   "source": [
    "Now, we can see `stream_mode=\"values\"`.\n",
    "\n",
    "This is the `full state` of the graph after the `conversation` node is called."
   ]
  },
  {
   "cell_type": "code",
   "id": "6ee763f8-6d1f-491e-8050-fb1439e116df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T07:16:16.926287Z",
     "start_time": "2025-08-16T07:16:15.795003Z"
    }
   },
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"hi! I'm Lance\")\n",
    "for event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    for m in event['messages']:\n",
    "        m.pretty_print()\n",
    "    print(\"---\"*25)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "hi! I'm Lance\n",
      "---------------------------------------------------------------------------\n",
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "hi! I'm Lance\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Hello, Lance! How can I assist you today?\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "563c198a-d1a4-4700-b7a7-ff5b8e0b25d7",
   "metadata": {},
   "source": [
    "### Streaming tokens\n",
    "\n",
    "We often want to stream more than graph state.\n",
    "\n",
    "In particular, with chat model calls it is common to stream the tokens as they are generated.\n",
    "\n",
    "We can do this [using the `.astream_events` method](https://langchain-ai.github.io/langgraph/how-tos/streaming-from-final-node/#stream-outputs-from-the-final-node), which streams back events as they happen inside nodes!\n",
    "\n",
    "Each event is a dict with a few keys:\n",
    " \n",
    "* `event`: This is the type of event that is being emitted. \n",
    "* `name`: This is the name of event.\n",
    "* `data`: This is the data associated with the event.\n",
    "* `metadata`: Contains`langgraph_node`, the node emitting the event.\n",
    "\n",
    "Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "id": "6ae8c7a6-c6e7-4cef-ac9f-190d2f4dd763",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T07:16:27.323170Z",
     "start_time": "2025-08-16T07:16:20.662963Z"
    }
   },
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    print(f\"Node: {event['metadata'].get('langgraph_node','')}. Type: {event['event']}. Name: {event['name']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: . Type: on_chain_start. Name: LangGraph\n",
      "Node: conversation. Type: on_chain_start. Name: conversation\n",
      "Node: conversation. Type: on_chat_model_start. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_end. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chain_start. Name: should_continue\n",
      "Node: conversation. Type: on_chain_end. Name: should_continue\n",
      "Node: conversation. Type: on_chain_stream. Name: conversation\n",
      "Node: conversation. Type: on_chain_end. Name: conversation\n",
      "Node: . Type: on_chain_stream. Name: LangGraph\n",
      "Node: . Type: on_chain_end. Name: LangGraph\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "0b63490f-3d24-4f68-95ca-5320ccb61d2d",
   "metadata": {},
   "source": [
    "The central point is that tokens from chat models within your graph have the `on_chat_model_stream` type.\n",
    "\n",
    "We can use `event['metadata']['langgraph_node']` to select the node to stream from.\n",
    "\n",
    "And we can use `event['data']` to get the actual data for each event, which in this case is an `AIMessageChunk`. "
   ]
  },
  {
   "cell_type": "code",
   "id": "cc3529f8-3960-4d41-9ed6-373f93183950",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T07:16:36.344304Z",
     "start_time": "2025-08-16T07:16:30.206200Z"
    }
   },
   "source": [
    "node_to_stream = 'conversation'\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    # Get chat model tokens from a particular node \n",
    "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
    "        print(event[\"data\"])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='The', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' San', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Francisco', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' are', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' professional', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' American', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' football', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' based', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' San', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Francisco', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Bay', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Area', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' They', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' compete', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' National', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Football', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' League', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='NFL', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=')', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' member', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' club', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' league', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=\"'s\", additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' National', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Football', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Conference', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='N', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='FC', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=')', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' West', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' division', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' was', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' founded', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='194', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='6', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' charter', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' member', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' All', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='-Amer', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='ica', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Football', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Conference', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='AA', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='FC', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=')', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' joined', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' NFL', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='194', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='9', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' when', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' leagues', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' merged', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='###', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Key', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Points', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=':\\n\\n', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='St', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='adium', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' play', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' home', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' games', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' at', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Levi', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=\"'s\", additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Stadium', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Santa', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Clara', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' California', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' which', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' they', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' moved', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='201', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='4', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Before', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' that', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' they', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' played', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' at', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Cand', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='lestick', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Park', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' San', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Francisco', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='Team', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Colors', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=\" team's\", additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' colors', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' are', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' red', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' gold', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' white', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='Champ', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='ionship', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' won', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' five', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Super', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' championships', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='Super', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' XVI', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' XIX', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' XX', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='III', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' XX', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='IV', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' XX', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='IX', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='),', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' most', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' successful', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' period', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' being', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='198', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='0', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' early', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='199', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='0', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='Not', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='able', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Figures', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' \\n', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' -', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='Joe', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Montana', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Often', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' considered', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' one', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' greatest', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' quarterbacks', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' NFL', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' history', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Montana', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' led', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' four', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Super', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' victories', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='.\\n', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' -', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='Jerry', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Rice', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Wid', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='ely', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' regarded', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' greatest', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' wide', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' receiver', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' NFL', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' history', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Rice', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' played', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' significant', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' role', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=\" team's\", additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' success', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' during', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' his', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' tenure', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='.\\n', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' -', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='Steve', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Young', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Another', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Hall', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Fame', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' quarterback', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' who', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' led', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Super', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' victory', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='199', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='4', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' season', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='.\\n', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' -', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='Bill', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Walsh', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' legendary', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' head', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' coach', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' who', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' introduced', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' West', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Coast', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' offense', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' leading', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' much', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=\" team's\", additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' success', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='198', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='0', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='R', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='ival', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='ries', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' notable', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' rival', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='ries', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Dallas', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Cowboys', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Los', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Angeles', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Rams', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Seattle', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Seahawks', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' among', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' others', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='Recent', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Performance', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' In', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' recent', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' years', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' experienced', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' resurgence', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' including', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Super', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' appearance', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='201', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='9', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' season', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' where', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' they', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' were', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' defeated', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' by', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Kansas', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' City', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Chiefs', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Super', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' LIV', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='The', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' are', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' known', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' for', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' rich', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' history', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' passionate', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' fan', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' base', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' significant', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' contributions', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' NFL', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' both', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' terms', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' on', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='-field', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' success', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' development', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' innovative', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' playing', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content=' styles', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n",
      "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'service_tier': 'default'}, id='run--62660b64-5bbf-49c7-96df-a4c8831376c7')}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "226e569a-76c3-43d8-8f89-3ae687efde1c",
   "metadata": {},
   "source": [
    "As you see above, just use the `chunk` key to get the `AIMessageChunk`."
   ]
  },
  {
   "cell_type": "code",
   "id": "3aeae53d-6dcf-40d0-a0c6-c40de492cc83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T07:16:44.290317Z",
     "start_time": "2025-08-16T07:16:39.117863Z"
    }
   },
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    # Get chat model tokens from a particular node \n",
    "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
    "        data = event[\"data\"]\n",
    "        print(data[\"chunk\"].content, end=\"|\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|The| San| Francisco| |49|ers| are| a| professional| American| football| team| based| in| the| San| Francisco| Bay| Area|.| They| compete| in| the| National| Football| League| (|NFL|)| as| a| member| club| of| the| league|'s| National| Football| Conference| (|N|FC|)| West| division|.| The| team| was| founded| in| |194|6| as| a| charter| member| of| the| All|-Amer|ica| Football| Conference| (|AA|FC|)| and| joined| the| NFL| in| |194|9| when| the| leagues| merged|.\n",
      "\n",
      "|###| Key| Points|:\n",
      "\n",
      "|-| **|Team| Name| and| Colors|**|:| The| team| is| named| after| the| prospect|ors| who| arrived| in| Northern| California| during| the| |184|9| Gold| Rush|.| The| team's| colors| are| red|,| gold|,| and| white|.\n",
      "\n",
      "|-| **|St|adium|**|:| The| |49|ers| play| their| home| games| at| Levi|'s| Stadium| in| Santa| Clara|,| California|,| which| they| moved| to| in| |201|4|.| Before| that|,| they| played| at| Cand|lestick| Park| in| San| Francisco|.\n",
      "\n",
      "|-| **|Champ|ionship|s|**|:| The| |49|ers| have| won| five| Super| Bowl| titles| (|X|VI|,| XIX|,| XX|III|,| XX|IV|,| and| XX|IX|),| with| their| most| successful| period| being| the| |198|0|s| and| early| |199|0|s|.| They| have| also| won| numerous| division| titles| and| conference| championships|.\n",
      "\n",
      "|-| **|Not|able| Figures|**|:| The| team| has| had| several| Hall| of| Fame| players|,| including| Joe| Montana|,| Jerry| Rice|,| Steve| Young|,| Ronnie| L|ott|,| and| Charles| Haley|.| Bill| Walsh|,| the| legendary| head| coach|,| is| credited| with| developing| the| West| Coast| offense|,| which| became| a| staple| of| the| team's| success|.\n",
      "\n",
      "|-| **|R|ival|ries|**|:| The| |49|ers| have| notable| rival|ries| with| the| Seattle| Seahawks|,| Los| Angeles| Rams|,| and| historically| with| the| Dallas| Cowboys| and| Green| Bay| Packers|.\n",
      "\n",
      "|-| **|Recent| Performance|**|:| In| recent| years|,| the| |49|ers| have| experienced| both| ups| and| downs|.| They| reached| the| Super| Bowl| in| the| |201|9| season| but| lost| to| the| Kansas| City| Chiefs|.| The| team| has| been| competitive| in| the| NFC| West|,| often| cont|ending| for| playoff| spots|.\n",
      "\n",
      "|-| **|Ownership| and| Management|**|:| The| team| is| owned| by| the| York| family|,| with| Jed| York| serving| as| the| CEO|.| The| general| manager| is| John| Lynch|,| and| the| head| coach| is| Kyle| Shan|ahan|,| known| for| his| offensive| ac|umen|.\n",
      "\n",
      "|The| |49|ers| are| known| for| their| rich| history|,| passionate| fan| base|,| and| significant| contributions| to| the| NFL|'s| development| and| popularity|.||"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5826e4d8-846b-4f6c-a5c1-e781d43022db",
   "metadata": {},
   "source": [
    "### Streaming with LangGraph API\n",
    "\n",
    "**⚠️ DISCLAIMER**\n",
    "\n",
    "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "You should see the following output:\n",
    "```\n",
    "- 🚀 API: http://127.0.0.1:2024\n",
    "- 🎨 Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "- 📚 API Docs: http://127.0.0.1:2024/docs\n",
    "```\n",
    "\n",
    "Open your browser and navigate to the Studio UI: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`.\n",
    "\n",
    "The LangGraph API [supports editing graph state](https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_edit_state/#initial-invocation). "
   ]
  },
  {
   "cell_type": "code",
   "id": "8925b632-512b-48e1-9220-61c06bfbf0b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T07:16:48.672352Z",
     "start_time": "2025-08-16T07:16:48.669707Z"
    }
   },
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab\")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "079c2ad6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T07:16:58.559569Z",
     "start_time": "2025-08-16T07:16:50.734174Z"
    }
   },
   "source": [
    "from langgraph_sdk import get_client\n",
    "\n",
    "# This is the URL of the local development server\n",
    "URL = \"http://127.0.0.1:2024\"\n",
    "client = get_client(url=URL)\n",
    "\n",
    "# Search all hosted graphs\n",
    "assistants = await client.assistants.search()"
   ],
   "outputs": [
    {
     "ename": "ConnectError",
     "evalue": "All connection attempts failed",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mConnectError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:101\u001B[39m, in \u001B[36mmap_httpcore_exceptions\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    100\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m101\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:394\u001B[39m, in \u001B[36mAsyncHTTPTransport.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    393\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[32m--> \u001B[39m\u001B[32m394\u001B[39m     resp = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pool.handle_async_request(req)\n\u001B[32m    396\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpcore/_async/connection_pool.py:256\u001B[39m, in \u001B[36mAsyncConnectionPool.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    255\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._close_connections(closing)\n\u001B[32m--> \u001B[39m\u001B[32m256\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    258\u001B[39m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[32m    259\u001B[39m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpcore/_async/connection_pool.py:236\u001B[39m, in \u001B[36mAsyncConnectionPool.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    234\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    235\u001B[39m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m236\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m connection.handle_async_request(\n\u001B[32m    237\u001B[39m         pool_request.request\n\u001B[32m    238\u001B[39m     )\n\u001B[32m    239\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[32m    240\u001B[39m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[32m    241\u001B[39m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[32m    242\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    243\u001B[39m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpcore/_async/connection.py:101\u001B[39m, in \u001B[36mAsyncHTTPConnection.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    100\u001B[39m     \u001B[38;5;28mself\u001B[39m._connect_failed = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m101\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[32m    103\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._connection.handle_async_request(request)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpcore/_async/connection.py:78\u001B[39m, in \u001B[36mAsyncHTTPConnection.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m     77\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m78\u001B[39m     stream = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._connect(request)\n\u001B[32m     80\u001B[39m     ssl_object = stream.get_extra_info(\u001B[33m\"\u001B[39m\u001B[33mssl_object\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpcore/_async/connection.py:124\u001B[39m, in \u001B[36mAsyncHTTPConnection._connect\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    123\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[33m\"\u001B[39m\u001B[33mconnect_tcp\u001B[39m\u001B[33m\"\u001B[39m, logger, request, kwargs) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[32m--> \u001B[39m\u001B[32m124\u001B[39m     stream = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._network_backend.connect_tcp(**kwargs)\n\u001B[32m    125\u001B[39m     trace.return_value = stream\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpcore/_backends/auto.py:31\u001B[39m, in \u001B[36mAutoBackend.connect_tcp\u001B[39m\u001B[34m(self, host, port, timeout, local_address, socket_options)\u001B[39m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._init_backend()\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backend.connect_tcp(\n\u001B[32m     32\u001B[39m     host,\n\u001B[32m     33\u001B[39m     port,\n\u001B[32m     34\u001B[39m     timeout=timeout,\n\u001B[32m     35\u001B[39m     local_address=local_address,\n\u001B[32m     36\u001B[39m     socket_options=socket_options,\n\u001B[32m     37\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpcore/_backends/anyio.py:113\u001B[39m, in \u001B[36mAnyIOBackend.connect_tcp\u001B[39m\u001B[34m(self, host, port, timeout, local_address, socket_options)\u001B[39m\n\u001B[32m    108\u001B[39m exc_map = {\n\u001B[32m    109\u001B[39m     \u001B[38;5;167;01mTimeoutError\u001B[39;00m: ConnectTimeout,\n\u001B[32m    110\u001B[39m     \u001B[38;5;167;01mOSError\u001B[39;00m: ConnectError,\n\u001B[32m    111\u001B[39m     anyio.BrokenResourceError: ConnectError,\n\u001B[32m    112\u001B[39m }\n\u001B[32m--> \u001B[39m\u001B[32m113\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[32m    114\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m anyio.fail_after(timeout):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py:162\u001B[39m, in \u001B[36m_GeneratorContextManager.__exit__\u001B[39m\u001B[34m(self, typ, value, traceback)\u001B[39m\n\u001B[32m    161\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m162\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgen\u001B[49m\u001B[43m.\u001B[49m\u001B[43mthrow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    163\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    164\u001B[39m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[32m    165\u001B[39m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[32m    166\u001B[39m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpcore/_exceptions.py:14\u001B[39m, in \u001B[36mmap_exceptions\u001B[39m\u001B[34m(map)\u001B[39m\n\u001B[32m     13\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(exc, from_exc):\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m to_exc(exc) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mexc\u001B[39;00m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[31mConnectError\u001B[39m: All connection attempts failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mConnectError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      5\u001B[39m client = get_client(url=URL)\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# Search all hosted graphs\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m assistants = \u001B[38;5;28;01mawait\u001B[39;00m client.assistants.search()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/langgraph_sdk/client.py:913\u001B[39m, in \u001B[36mAssistantsClient.search\u001B[39m\u001B[34m(self, metadata, graph_id, limit, offset, sort_by, sort_order, headers)\u001B[39m\n\u001B[32m    911\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m sort_order:\n\u001B[32m    912\u001B[39m     payload[\u001B[33m\"\u001B[39m\u001B[33msort_order\u001B[39m\u001B[33m\"\u001B[39m] = sort_order\n\u001B[32m--> \u001B[39m\u001B[32m913\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.http.post(\n\u001B[32m    914\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m/assistants/search\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    915\u001B[39m     json=payload,\n\u001B[32m    916\u001B[39m     headers=headers,\n\u001B[32m    917\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/langgraph_sdk/client.py:288\u001B[39m, in \u001B[36mHttpClient.post\u001B[39m\u001B[34m(self, path, json, headers, on_response)\u001B[39m\n\u001B[32m    286\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m headers:\n\u001B[32m    287\u001B[39m     request_headers.update(headers)\n\u001B[32m--> \u001B[39m\u001B[32m288\u001B[39m r = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.client.post(path, headers=request_headers, content=content)\n\u001B[32m    289\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m on_response:\n\u001B[32m    290\u001B[39m     on_response(r)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_client.py:1859\u001B[39m, in \u001B[36mAsyncClient.post\u001B[39m\u001B[34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001B[39m\n\u001B[32m   1838\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1839\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1840\u001B[39m     url: URL | \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1852\u001B[39m     extensions: RequestExtensions | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1853\u001B[39m ) -> Response:\n\u001B[32m   1854\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1855\u001B[39m \u001B[33;03m    Send a `POST` request.\u001B[39;00m\n\u001B[32m   1856\u001B[39m \n\u001B[32m   1857\u001B[39m \u001B[33;03m    **Parameters**: See `httpx.request`.\u001B[39;00m\n\u001B[32m   1858\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1859\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.request(\n\u001B[32m   1860\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mPOST\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1861\u001B[39m         url,\n\u001B[32m   1862\u001B[39m         content=content,\n\u001B[32m   1863\u001B[39m         data=data,\n\u001B[32m   1864\u001B[39m         files=files,\n\u001B[32m   1865\u001B[39m         json=json,\n\u001B[32m   1866\u001B[39m         params=params,\n\u001B[32m   1867\u001B[39m         headers=headers,\n\u001B[32m   1868\u001B[39m         cookies=cookies,\n\u001B[32m   1869\u001B[39m         auth=auth,\n\u001B[32m   1870\u001B[39m         follow_redirects=follow_redirects,\n\u001B[32m   1871\u001B[39m         timeout=timeout,\n\u001B[32m   1872\u001B[39m         extensions=extensions,\n\u001B[32m   1873\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_client.py:1540\u001B[39m, in \u001B[36mAsyncClient.request\u001B[39m\u001B[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001B[39m\n\u001B[32m   1525\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m   1527\u001B[39m request = \u001B[38;5;28mself\u001B[39m.build_request(\n\u001B[32m   1528\u001B[39m     method=method,\n\u001B[32m   1529\u001B[39m     url=url,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1538\u001B[39m     extensions=extensions,\n\u001B[32m   1539\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1540\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_client.py:1629\u001B[39m, in \u001B[36mAsyncClient.send\u001B[39m\u001B[34m(self, request, stream, auth, follow_redirects)\u001B[39m\n\u001B[32m   1625\u001B[39m \u001B[38;5;28mself\u001B[39m._set_timeout(request)\n\u001B[32m   1627\u001B[39m auth = \u001B[38;5;28mself\u001B[39m._build_request_auth(request, auth)\n\u001B[32m-> \u001B[39m\u001B[32m1629\u001B[39m response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._send_handling_auth(\n\u001B[32m   1630\u001B[39m     request,\n\u001B[32m   1631\u001B[39m     auth=auth,\n\u001B[32m   1632\u001B[39m     follow_redirects=follow_redirects,\n\u001B[32m   1633\u001B[39m     history=[],\n\u001B[32m   1634\u001B[39m )\n\u001B[32m   1635\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1636\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_client.py:1657\u001B[39m, in \u001B[36mAsyncClient._send_handling_auth\u001B[39m\u001B[34m(self, request, auth, follow_redirects, history)\u001B[39m\n\u001B[32m   1654\u001B[39m request = \u001B[38;5;28;01mawait\u001B[39;00m auth_flow.\u001B[34m__anext__\u001B[39m()\n\u001B[32m   1656\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1657\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._send_handling_redirects(\n\u001B[32m   1658\u001B[39m         request,\n\u001B[32m   1659\u001B[39m         follow_redirects=follow_redirects,\n\u001B[32m   1660\u001B[39m         history=history,\n\u001B[32m   1661\u001B[39m     )\n\u001B[32m   1662\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1663\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_client.py:1694\u001B[39m, in \u001B[36mAsyncClient._send_handling_redirects\u001B[39m\u001B[34m(self, request, follow_redirects, history)\u001B[39m\n\u001B[32m   1691\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._event_hooks[\u001B[33m\"\u001B[39m\u001B[33mrequest\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1692\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m hook(request)\n\u001B[32m-> \u001B[39m\u001B[32m1694\u001B[39m response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._send_single_request(request)\n\u001B[32m   1695\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1696\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._event_hooks[\u001B[33m\"\u001B[39m\u001B[33mresponse\u001B[39m\u001B[33m\"\u001B[39m]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_client.py:1730\u001B[39m, in \u001B[36mAsyncClient._send_single_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m   1725\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   1726\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAttempted to send an sync request with an AsyncClient instance.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1727\u001B[39m     )\n\u001B[32m   1729\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=request):\n\u001B[32m-> \u001B[39m\u001B[32m1730\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m transport.handle_async_request(request)\n\u001B[32m   1732\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response.stream, AsyncByteStream)\n\u001B[32m   1733\u001B[39m response.request = request\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:393\u001B[39m, in \u001B[36mAsyncHTTPTransport.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    379\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mhttpcore\u001B[39;00m\n\u001B[32m    381\u001B[39m req = httpcore.Request(\n\u001B[32m    382\u001B[39m     method=request.method,\n\u001B[32m    383\u001B[39m     url=httpcore.URL(\n\u001B[32m   (...)\u001B[39m\u001B[32m    391\u001B[39m     extensions=request.extensions,\n\u001B[32m    392\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m393\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[32m    394\u001B[39m     resp = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pool.handle_async_request(req)\n\u001B[32m    396\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py:162\u001B[39m, in \u001B[36m_GeneratorContextManager.__exit__\u001B[39m\u001B[34m(self, typ, value, traceback)\u001B[39m\n\u001B[32m    160\u001B[39m     value = typ()\n\u001B[32m    161\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m162\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgen\u001B[49m\u001B[43m.\u001B[49m\u001B[43mthrow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    163\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    164\u001B[39m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[32m    165\u001B[39m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[32m    166\u001B[39m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n\u001B[32m    167\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m exc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m value\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:118\u001B[39m, in \u001B[36mmap_httpcore_exceptions\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m    117\u001B[39m message = \u001B[38;5;28mstr\u001B[39m(exc)\n\u001B[32m--> \u001B[39m\u001B[32m118\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m mapped_exc(message) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mexc\u001B[39;00m\n",
      "\u001B[31mConnectError\u001B[39m: All connection attempts failed"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "4d15af9e-0e86-41e3-a5ba-ee2a4aa08a32",
   "metadata": {},
   "source": [
    "Let's [stream `values`](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_values/), like before."
   ]
  },
  {
   "cell_type": "code",
   "id": "63e3096f-5429-4d3c-8de2-2bddf7266ebf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T07:17:16.850824Z",
     "start_time": "2025-08-16T07:17:09.202662Z"
    }
   },
   "source": [
    "# Create a new thread\n",
    "thread = await client.threads.create()\n",
    "# Input message\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"agent\", \n",
    "                                      input={\"messages\": [input_message]}, \n",
    "                                      stream_mode=\"values\"):\n",
    "    print(event)"
   ],
   "outputs": [
    {
     "ename": "ConnectError",
     "evalue": "All connection attempts failed",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mConnectError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:101\u001B[39m, in \u001B[36mmap_httpcore_exceptions\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    100\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m101\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:394\u001B[39m, in \u001B[36mAsyncHTTPTransport.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    393\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[32m--> \u001B[39m\u001B[32m394\u001B[39m     resp = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pool.handle_async_request(req)\n\u001B[32m    396\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpcore/_async/connection_pool.py:256\u001B[39m, in \u001B[36mAsyncConnectionPool.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    255\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._close_connections(closing)\n\u001B[32m--> \u001B[39m\u001B[32m256\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    258\u001B[39m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[32m    259\u001B[39m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpcore/_async/connection_pool.py:236\u001B[39m, in \u001B[36mAsyncConnectionPool.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    234\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    235\u001B[39m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m236\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m connection.handle_async_request(\n\u001B[32m    237\u001B[39m         pool_request.request\n\u001B[32m    238\u001B[39m     )\n\u001B[32m    239\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[32m    240\u001B[39m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[32m    241\u001B[39m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[32m    242\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    243\u001B[39m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpcore/_async/connection.py:101\u001B[39m, in \u001B[36mAsyncHTTPConnection.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    100\u001B[39m     \u001B[38;5;28mself\u001B[39m._connect_failed = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m101\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[32m    103\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._connection.handle_async_request(request)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpcore/_async/connection.py:78\u001B[39m, in \u001B[36mAsyncHTTPConnection.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m     77\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m78\u001B[39m     stream = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._connect(request)\n\u001B[32m     80\u001B[39m     ssl_object = stream.get_extra_info(\u001B[33m\"\u001B[39m\u001B[33mssl_object\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpcore/_async/connection.py:124\u001B[39m, in \u001B[36mAsyncHTTPConnection._connect\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    123\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[33m\"\u001B[39m\u001B[33mconnect_tcp\u001B[39m\u001B[33m\"\u001B[39m, logger, request, kwargs) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[32m--> \u001B[39m\u001B[32m124\u001B[39m     stream = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._network_backend.connect_tcp(**kwargs)\n\u001B[32m    125\u001B[39m     trace.return_value = stream\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpcore/_backends/auto.py:31\u001B[39m, in \u001B[36mAutoBackend.connect_tcp\u001B[39m\u001B[34m(self, host, port, timeout, local_address, socket_options)\u001B[39m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._init_backend()\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backend.connect_tcp(\n\u001B[32m     32\u001B[39m     host,\n\u001B[32m     33\u001B[39m     port,\n\u001B[32m     34\u001B[39m     timeout=timeout,\n\u001B[32m     35\u001B[39m     local_address=local_address,\n\u001B[32m     36\u001B[39m     socket_options=socket_options,\n\u001B[32m     37\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpcore/_backends/anyio.py:113\u001B[39m, in \u001B[36mAnyIOBackend.connect_tcp\u001B[39m\u001B[34m(self, host, port, timeout, local_address, socket_options)\u001B[39m\n\u001B[32m    108\u001B[39m exc_map = {\n\u001B[32m    109\u001B[39m     \u001B[38;5;167;01mTimeoutError\u001B[39;00m: ConnectTimeout,\n\u001B[32m    110\u001B[39m     \u001B[38;5;167;01mOSError\u001B[39;00m: ConnectError,\n\u001B[32m    111\u001B[39m     anyio.BrokenResourceError: ConnectError,\n\u001B[32m    112\u001B[39m }\n\u001B[32m--> \u001B[39m\u001B[32m113\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[32m    114\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m anyio.fail_after(timeout):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py:162\u001B[39m, in \u001B[36m_GeneratorContextManager.__exit__\u001B[39m\u001B[34m(self, typ, value, traceback)\u001B[39m\n\u001B[32m    161\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m162\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgen\u001B[49m\u001B[43m.\u001B[49m\u001B[43mthrow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    163\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    164\u001B[39m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[32m    165\u001B[39m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[32m    166\u001B[39m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpcore/_exceptions.py:14\u001B[39m, in \u001B[36mmap_exceptions\u001B[39m\u001B[34m(map)\u001B[39m\n\u001B[32m     13\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(exc, from_exc):\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m to_exc(exc) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mexc\u001B[39;00m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[31mConnectError\u001B[39m: All connection attempts failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mConnectError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Create a new thread\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m thread = \u001B[38;5;28;01mawait\u001B[39;00m client.threads.create()\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# Input message\u001B[39;00m\n\u001B[32m      4\u001B[39m input_message = HumanMessage(content=\u001B[33m\"\u001B[39m\u001B[33mMultiply 2 and 3\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/langgraph_sdk/client.py:1113\u001B[39m, in \u001B[36mThreadsClient.create\u001B[39m\u001B[34m(self, metadata, thread_id, if_exists, supersteps, graph_id, headers)\u001B[39m\n\u001B[32m   1098\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m supersteps:\n\u001B[32m   1099\u001B[39m     payload[\u001B[33m\"\u001B[39m\u001B[33msupersteps\u001B[39m\u001B[33m\"\u001B[39m] = [\n\u001B[32m   1100\u001B[39m         {\n\u001B[32m   1101\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mupdates\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m   (...)\u001B[39m\u001B[32m   1110\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m supersteps\n\u001B[32m   1111\u001B[39m     ]\n\u001B[32m-> \u001B[39m\u001B[32m1113\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.http.post(\u001B[33m\"\u001B[39m\u001B[33m/threads\u001B[39m\u001B[33m\"\u001B[39m, json=payload, headers=headers)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/langgraph_sdk/client.py:288\u001B[39m, in \u001B[36mHttpClient.post\u001B[39m\u001B[34m(self, path, json, headers, on_response)\u001B[39m\n\u001B[32m    286\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m headers:\n\u001B[32m    287\u001B[39m     request_headers.update(headers)\n\u001B[32m--> \u001B[39m\u001B[32m288\u001B[39m r = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.client.post(path, headers=request_headers, content=content)\n\u001B[32m    289\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m on_response:\n\u001B[32m    290\u001B[39m     on_response(r)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_client.py:1859\u001B[39m, in \u001B[36mAsyncClient.post\u001B[39m\u001B[34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001B[39m\n\u001B[32m   1838\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1839\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1840\u001B[39m     url: URL | \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1852\u001B[39m     extensions: RequestExtensions | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1853\u001B[39m ) -> Response:\n\u001B[32m   1854\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1855\u001B[39m \u001B[33;03m    Send a `POST` request.\u001B[39;00m\n\u001B[32m   1856\u001B[39m \n\u001B[32m   1857\u001B[39m \u001B[33;03m    **Parameters**: See `httpx.request`.\u001B[39;00m\n\u001B[32m   1858\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1859\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.request(\n\u001B[32m   1860\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mPOST\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1861\u001B[39m         url,\n\u001B[32m   1862\u001B[39m         content=content,\n\u001B[32m   1863\u001B[39m         data=data,\n\u001B[32m   1864\u001B[39m         files=files,\n\u001B[32m   1865\u001B[39m         json=json,\n\u001B[32m   1866\u001B[39m         params=params,\n\u001B[32m   1867\u001B[39m         headers=headers,\n\u001B[32m   1868\u001B[39m         cookies=cookies,\n\u001B[32m   1869\u001B[39m         auth=auth,\n\u001B[32m   1870\u001B[39m         follow_redirects=follow_redirects,\n\u001B[32m   1871\u001B[39m         timeout=timeout,\n\u001B[32m   1872\u001B[39m         extensions=extensions,\n\u001B[32m   1873\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_client.py:1540\u001B[39m, in \u001B[36mAsyncClient.request\u001B[39m\u001B[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001B[39m\n\u001B[32m   1525\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m   1527\u001B[39m request = \u001B[38;5;28mself\u001B[39m.build_request(\n\u001B[32m   1528\u001B[39m     method=method,\n\u001B[32m   1529\u001B[39m     url=url,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1538\u001B[39m     extensions=extensions,\n\u001B[32m   1539\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1540\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_client.py:1629\u001B[39m, in \u001B[36mAsyncClient.send\u001B[39m\u001B[34m(self, request, stream, auth, follow_redirects)\u001B[39m\n\u001B[32m   1625\u001B[39m \u001B[38;5;28mself\u001B[39m._set_timeout(request)\n\u001B[32m   1627\u001B[39m auth = \u001B[38;5;28mself\u001B[39m._build_request_auth(request, auth)\n\u001B[32m-> \u001B[39m\u001B[32m1629\u001B[39m response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._send_handling_auth(\n\u001B[32m   1630\u001B[39m     request,\n\u001B[32m   1631\u001B[39m     auth=auth,\n\u001B[32m   1632\u001B[39m     follow_redirects=follow_redirects,\n\u001B[32m   1633\u001B[39m     history=[],\n\u001B[32m   1634\u001B[39m )\n\u001B[32m   1635\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1636\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_client.py:1657\u001B[39m, in \u001B[36mAsyncClient._send_handling_auth\u001B[39m\u001B[34m(self, request, auth, follow_redirects, history)\u001B[39m\n\u001B[32m   1654\u001B[39m request = \u001B[38;5;28;01mawait\u001B[39;00m auth_flow.\u001B[34m__anext__\u001B[39m()\n\u001B[32m   1656\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1657\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._send_handling_redirects(\n\u001B[32m   1658\u001B[39m         request,\n\u001B[32m   1659\u001B[39m         follow_redirects=follow_redirects,\n\u001B[32m   1660\u001B[39m         history=history,\n\u001B[32m   1661\u001B[39m     )\n\u001B[32m   1662\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1663\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_client.py:1694\u001B[39m, in \u001B[36mAsyncClient._send_handling_redirects\u001B[39m\u001B[34m(self, request, follow_redirects, history)\u001B[39m\n\u001B[32m   1691\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._event_hooks[\u001B[33m\"\u001B[39m\u001B[33mrequest\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1692\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m hook(request)\n\u001B[32m-> \u001B[39m\u001B[32m1694\u001B[39m response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._send_single_request(request)\n\u001B[32m   1695\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1696\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._event_hooks[\u001B[33m\"\u001B[39m\u001B[33mresponse\u001B[39m\u001B[33m\"\u001B[39m]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_client.py:1730\u001B[39m, in \u001B[36mAsyncClient._send_single_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m   1725\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   1726\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAttempted to send an sync request with an AsyncClient instance.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1727\u001B[39m     )\n\u001B[32m   1729\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=request):\n\u001B[32m-> \u001B[39m\u001B[32m1730\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m transport.handle_async_request(request)\n\u001B[32m   1732\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response.stream, AsyncByteStream)\n\u001B[32m   1733\u001B[39m response.request = request\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:393\u001B[39m, in \u001B[36mAsyncHTTPTransport.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    379\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mhttpcore\u001B[39;00m\n\u001B[32m    381\u001B[39m req = httpcore.Request(\n\u001B[32m    382\u001B[39m     method=request.method,\n\u001B[32m    383\u001B[39m     url=httpcore.URL(\n\u001B[32m   (...)\u001B[39m\u001B[32m    391\u001B[39m     extensions=request.extensions,\n\u001B[32m    392\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m393\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[32m    394\u001B[39m     resp = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pool.handle_async_request(req)\n\u001B[32m    396\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py:162\u001B[39m, in \u001B[36m_GeneratorContextManager.__exit__\u001B[39m\u001B[34m(self, typ, value, traceback)\u001B[39m\n\u001B[32m    160\u001B[39m     value = typ()\n\u001B[32m    161\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m162\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgen\u001B[49m\u001B[43m.\u001B[49m\u001B[43mthrow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    163\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    164\u001B[39m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[32m    165\u001B[39m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[32m    166\u001B[39m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n\u001B[32m    167\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m exc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m value\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/orchestrator/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:118\u001B[39m, in \u001B[36mmap_httpcore_exceptions\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m    117\u001B[39m message = \u001B[38;5;28mstr\u001B[39m(exc)\n\u001B[32m--> \u001B[39m\u001B[32m118\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m mapped_exc(message) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mexc\u001B[39;00m\n",
      "\u001B[31mConnectError\u001B[39m: All connection attempts failed"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "556dc7fd-1cae-404f-816a-f13d772b3b14",
   "metadata": {},
   "source": [
    "The streamed objects have: \n",
    "\n",
    "* `event`: Type\n",
    "* `data`: State"
   ]
  },
  {
   "cell_type": "code",
   "id": "57b735aa-139c-45a3-a850-63519c0004f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T04:35:38.063875Z",
     "start_time": "2025-07-31T04:35:35.140152Z"
    }
   },
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], assistant_id=\"agent\", input={\"messages\": [input_message]}, stream_mode=\"values\"):\n",
    "    messages = event.data.get('messages',None)\n",
    "    if messages:\n",
    "        print(convert_to_messages(messages)[-1])\n",
    "    print('='*25)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "content='Multiply 2 and 3' additional_kwargs={} response_metadata={} id='d7d923fe-1091-4224-b9c9-dadc6418a5af'\n",
      "=========================\n",
      "content='' additional_kwargs={'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 17, 'total_tokens': 151, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [{'id': 'call_bg0QZEDVRF8SjQg3QyiGuQcZ', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 134, 'total_tokens': 151, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-BzFIuAvmXPRWMUsHVCl6lVvsUnuh4', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--7746baa0-7942-441e-9018-ce7439e41f1a-0' tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_bg0QZEDVRF8SjQg3QyiGuQcZ', 'type': 'tool_call'}]\n",
      "=========================\n",
      "content='6' name='multiply' id='7f7b7496-4364-4f91-91df-9c3a775fa19a' tool_call_id='call_bg0QZEDVRF8SjQg3QyiGuQcZ'\n",
      "=========================\n",
      "content='The result of multiplying 2 and 3 is 6.' additional_kwargs={'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 159, 'output_tokens': 14, 'total_tokens': 173, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 159, 'total_tokens': 173, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-BzFIvnjvQ10qcjB3RubyB3koJvups', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--c8f771fe-a893-4839-8dfd-6aaffb3e45b5-0'\n",
      "=========================\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "a555d186-27be-4ddf-934c-895a3105035d",
   "metadata": {},
   "source": [
    "There are some new streaming mode that are only supported via the API.\n",
    "\n",
    "For example, we can [use `messages` mode](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_messages/) to better handle the above case!\n",
    "\n",
    "This mode currently assumes that you have a `messages` key in your graph, which is a list of messages.\n",
    "\n",
    "All events emitted using `messages` mode have two attributes:\n",
    "\n",
    "* `event`: This is the name of the event\n",
    "* `data`: This is data associated with the event"
   ]
  },
  {
   "cell_type": "code",
   "id": "4abd91f6-63c0-41ee-9988-7c8248b88a45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T04:35:44.428632Z",
     "start_time": "2025-07-31T04:35:42.001012Z"
    }
   },
   "source": [
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"agent\", \n",
    "                                      input={\"messages\": [input_message]}, \n",
    "                                      stream_mode=\"messages\"):\n",
    "    print(event.event)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata\n",
      "messages/metadata\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/metadata\n",
      "messages/complete\n",
      "messages/metadata\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "8de2f1ea-b232-43fc-af7a-320efce83381",
   "metadata": {},
   "source": [
    "We can see a few events: \n",
    "\n",
    "* `metadata`: metadata about the run\n",
    "* `messages/complete`: fully formed message \n",
    "* `messages/partial`: chat model tokens\n",
    "\n",
    "You can dig further into the types [here](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#modemessages).\n",
    "\n",
    "Now, let's show how to stream these messages. \n",
    "\n",
    "We'll define a helper function for better formatting of the tool calls in messages."
   ]
  },
  {
   "cell_type": "code",
   "id": "50a85e16-6e3f-4f14-bcf9-8889a762f522",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T04:35:51.302082Z",
     "start_time": "2025-07-31T04:35:47.706623Z"
    }
   },
   "source": [
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "\n",
    "def format_tool_calls(tool_calls):\n",
    "    \"\"\"\n",
    "    Format a list of tool calls into a readable string.\n",
    "\n",
    "    Args:\n",
    "        tool_calls (list): A list of dictionaries, each representing a tool call.\n",
    "            Each dictionary should have 'id', 'name', and 'args' keys.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string of tool calls, or \"No tool calls\" if the list is empty.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if tool_calls:\n",
    "        formatted_calls = []\n",
    "        for call in tool_calls:\n",
    "            formatted_calls.append(\n",
    "                f\"Tool Call ID: {call['id']}, Function: {call['name']}, Arguments: {call['args']}\"\n",
    "            )\n",
    "        return \"\\n\".join(formatted_calls)\n",
    "    return \"No tool calls\"\n",
    "\n",
    "async for event in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input={\"messages\": [input_message]},\n",
    "    stream_mode=\"messages\",):\n",
    "    \n",
    "    # Handle metadata events\n",
    "    if event.event == \"metadata\":\n",
    "        print(f\"Metadata: Run ID - {event.data['run_id']}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Handle partial message events\n",
    "    elif event.event == \"messages/partial\":\n",
    "        for data_item in event.data:\n",
    "            # Process user messages\n",
    "            if \"role\" in data_item and data_item[\"role\"] == \"user\":\n",
    "                print(f\"Human: {data_item['content']}\")\n",
    "            else:\n",
    "                # Extract relevant data from the event\n",
    "                tool_calls = data_item.get(\"tool_calls\", [])\n",
    "                invalid_tool_calls = data_item.get(\"invalid_tool_calls\", [])\n",
    "                content = data_item.get(\"content\", \"\")\n",
    "                response_metadata = data_item.get(\"response_metadata\", {})\n",
    "\n",
    "                if content:\n",
    "                    print(f\"AI: {content}\")\n",
    "\n",
    "                if tool_calls:\n",
    "                    print(\"Tool Calls:\")\n",
    "                    print(format_tool_calls(tool_calls))\n",
    "\n",
    "                if invalid_tool_calls:\n",
    "                    print(\"Invalid Tool Calls:\")\n",
    "                    print(format_tool_calls(invalid_tool_calls))\n",
    "\n",
    "                if response_metadata:\n",
    "                    finish_reason = response_metadata.get(\"finish_reason\", \"N/A\")\n",
    "                    print(f\"Response Metadata: Finish Reason - {finish_reason}\")\n",
    "                    \n",
    "        print(\"-\" * 50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: Run ID - 1f06dc7d-45b8-6d2a-9532-2a3dc910948c\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_RwJoJMzGDqL7f0psDXgcX9ev, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_RwJoJMzGDqL7f0psDXgcX9ev, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_RwJoJMzGDqL7f0psDXgcX9ev, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_RwJoJMzGDqL7f0psDXgcX9ev, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_RwJoJMzGDqL7f0psDXgcX9ev, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_RwJoJMzGDqL7f0psDXgcX9ev, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_RwJoJMzGDqL7f0psDXgcX9ev, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_RwJoJMzGDqL7f0psDXgcX9ev, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_RwJoJMzGDqL7f0psDXgcX9ev, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_RwJoJMzGDqL7f0psDXgcX9ev, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_RwJoJMzGDqL7f0psDXgcX9ev, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
      "Response Metadata: Finish Reason - tool_calls\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "AI: The\n",
      "--------------------------------------------------\n",
      "AI: The result\n",
      "--------------------------------------------------\n",
      "AI: The result of\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying \n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and \n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is \n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6.\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6.\n",
      "Response Metadata: Finish Reason - stop\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae885f8-102f-448a-9d68-8ded8d2bbd18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
